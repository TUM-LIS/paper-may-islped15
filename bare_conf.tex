\documentclass[conference]{IEEEtran}
\usepackage{graphicx}
\usepackage{soul}
\usepackage{todonotes}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}

\begin{document}

\title{Realization of Fine-grained Sequential Approximate Circuits using Fault Emulation}


\author{\IEEEauthorblockN{BLIND REVIEW}
\IEEEauthorblockA{}
\and
\IEEEauthorblockN{BLIND REVIEW}
\IEEEauthorblockA{}
}


% make the title area
\maketitle


\begin{abstract}
%\boldmath
Approximate Computing has recently drawn interest due to its promise to substantially decrease the power consumption of integrated circuits. By tolerating a certain incorrectness at a circuit output, it can be operated at a more resource-saving state. For instance, parts of the circuit could be switched off or driven at sub-threshold voltage. Clearly, not all applications are suitable for this approach. Especially applications from the signal and image processing domain are suitable, due to their intrinsic tolerance to imprecision. But even in these circuits, one has to be very careful when and where to approximate a circuit, in order not to violate a minimum QoS.\\
In this paper we are presenting an approach to generate approximate circuits from existing deterministic implementations. The flow reaches from application-driven QoS definition down to approximated RTL. We are using FPGA-based fault emulation of the circuit in order to find out how faults, i.e. imprecisions in the circuit, affect the circuit behavior.\\
Most existing approaches only consider combinational circuits. Compared to existing approaches considering sequential circuits, our approach is very fast and accurate due to the FPGA-based emulation. And furthermore, we are able to tune the resulting precision to the defined QoS, in order to bring out the best gain of the approximation.
\end{abstract}

\IEEEpeerreviewmaketitle



\section{Introduction}
Due to increasing demand for low-power applications during the last decade, energy-efficiency has become a major factor in embedded systems design. However, voltage-scaling, the most efficient technique to save power, can not be further applied in conventional systems. The sophisticated scaling of MOSFET feature-sizes, makes circuits prone to even small voltage variations, when operating in the near-threshold area, and hence susceptible to errors within the circuit. And furthermore, as a consequence thereof, the decreased operating frequency is not acceptable in many cases.
\subsubsection*{Approximate Computing}
One emerging approach to tackle this problem is captioned by the term \emph{Approximate Computing}. Approximate computing tries to increase the energy-efficiency by tolerating a certain uncertainty at the circuit outputs, i.e. the result of a operation. The term covers a variety of points of application, from programming level to transistor level, as well as techniques on how to reduce the power consumption. Power can be saved for instance by calculating with a \emph{reduced precision}, i.e. removing parts of the circuit, a method clearly related to fixed-point arithmetic. However, approximate computing introduces dynamics to this field. It proposes to switch off and on the precision depending on the actual demand of the application. Another way can be to \emph{over-scale the voltage}, and hence accept timing violations or nose-based faults. Another method, we want to explicitly mention here, is related to circuit soft-error reliability. Approximate computing can be used to reduce the hardware overhead due to fault tolerance mechanisms, e.g. by \emph{omitting redundancy} wherever it is not absolutely needed. Clearly this approach is not applicable for all kind of applications. Especially applications form the signal processing domain are suitable. These applications usually have an inherent imprecision tolerance. They have to deal with the human (imperfect) perception, have redundant input data in order to deal with noisy data. For a comprehensive coverage of the topic, we refer to the work of Han et al. \cite{han_approximate_2013}.
\subsubsection*{Motivational Example}
Consider any arbitrary wireless communication system, e.g. like IEEE 802.11 Wi-Fi. Like any similar system, 802.11 is as system, designed to deal with noise on the received radio signal. It is designed to transmit data error-free in a good, as well as in a bad radio channel. If the channel is noisy between two stations, the system switches to a lower-order modulations scheme, resulting in a lower data-rate, in order to be more robust. However, internally the hardware is working with the same precision independent on the signal quality, which might not be necessary. For instance, such systems usually consist of one or more channel-coding schemes. At the receiving end, the decoder extracts the payload from the received signal, not matter if it was noisy or not. Hence, if the SNR of the signal is high, the decoder could work less precise, requiring less power, and still extract the payload error-free. Depending on the signal quality such a dynamic approximate system could then switch between different power/precision states without having any influence on the performance.
\subsubsection*{Problem Description}
Even if the applications mentioned above seem to be applicable for approximate computing, parts of the circuit cannot be easily switched off and faults cannot be tolerated everywhere in the circuit. It is crucial to know which parts of the circuit influence the functionality in which way. Ideally, we want to get this knowledge for each gate in the circuit, to get a fine-grained information about the possible approximation. To simplify the problem we are considering circuits at register-transfer level, which gives us a good trade-off between the precision of gate-level and the abstraction of algorithmic level. However, even there the number of possible states of the circuit corresponds to $2^N$, where $N$ is the number of registers in the circuit. Hence, the difficulty of realizing approximate computing on circuit level is to efficiently and reliable determine which parts of the circuit can be approximated and to what extent.
\subsubsection*{Related Work}
As mentioned earlier, approximate computing is a research field that can, and in our opinion has be, considered at many levels. Furthermore, it can be applied to CPU based applications, as well as dedicated hardware accelerators. In the former case usually the source code is annotated defining which operations can be executed on approximate hardware, e.g. as proposed in \cite{sampson_enerj:_2011}. In the latter case, most research effort has been spend on building approximate standard building blocks, like adders and multipliers \cite{huang_exploring_2011,chakrapani_highly_2008,gupta_impact:_2011,kahng_accuracy-configurable_2012}. However, most methods presented are based on manual interaction. Only a few present an automated approach, like Nepal et al. that proposed a automatic method, based on the behavioral description. Some works focus on approximation of integrated circuits at synthesis time \cite{miao_approximate_2013,shin_approximate_2010,choudhury_approximate_2008}. However, only very few explicitly focus on the approximation of sequential circuits. Ranjan et al. \cite{ranjan_aslan:_2014} use formal verification techniques to identify the impact of approximate parts of a sequential circuit on the global output. This seems to be a very promising approach. However, it only offers the ability to generate fixed approximate circuits based on precision reduction. In order to save power e.g. using dynamic voltage overscaling, an even more fine-grained analysis is required, to model varying approximation operating points. The same applies for omitting redundancy. In order to emulate different radiation or aging levels, a more flexible model is required. \\
In order to solve these limitations we introduced bit-flip probabilities to the analysis. As already mentioned, we are focusing on register-transfer level. We are assigning bit-flip probabilities to each register in a circuit, in order to model the various effects responsible for the approximation: from radiation, via timing violations, to intentionally switching off parts of the circuit. We are using FPGA-based emulation to actually flip the bits. This makes the analysis very fast and allows us to cover a large state space, so that the results are reliable.
\subsubsection*{Contribution of this Work}
 In this work we are presenting and evaluating a methodology in order to find out which parts of a sequential circuit can be approximated and to what extent. The approach is automated wherever it is possible. Given a netlist and a set of quality constraints our tool automatically generates the information of what bit-flip probability is allowed at which register, without violating the output constraint. The approach is fine-granular. I.e. we can not only model the ``integer'' pruning of circuit elements, but also model a bit-flip probability. Integer approximations, i.e. if parts of the circuit can be pruned, can then be implemented directly. Non-integer information can then be used generate approximate combinational logic, e.g. based on aggressive power-scaling, using existing solutions.

The remainder of this paper is organized as follows: In Section \ref{sec:methodology} we are explaining our methodology in detail. In Section \ref{sec:experimental_results} we will evaluate our approach with various realistic benchmark circuits and compare the performance with other approaches. And finally, in Section \ref{sec:conclusion} we will conclude our work.





\section{Methodology}
\label{sec:methodology}
\subsection{Probability-aware Fault Emulation}
\begin{figure}[htb]
  \centering
  \missingfigure[figwidth=.5\textwidth]{Simulator block-diagram}
  \caption{Block-diagram of the FPGA-based fault emulation system}
  \label{fig:faultify_block}
\end{figure}
\hl{missing taht we compare cut and goc in every cycle!!!}
In our previous work we have developed an FPGA-based fault emulation system. A block diagram can be seen in Figure \ref{fig:faultify_block}. Emulation means that the circuit we want to analyze is mapped to an FPGA, no matter if it was intended for an ASIC or an FPGA as target technology. In order to make the emulation as realistic as possible, the circuits have to emulated, after being synthesized for their specific target technology. In case of an ASIC target, primitives of the ASIC library have to be replaced by corresponding FPGA primitives in order to map the circuit onto the emulating FPGA. In case of an FPGA target usually no adaptions have to be made due to the interoperability of third party synthesis tools concerning FPGA vendors and their generations. Faults, hence bit-flips, are injected at register level. By doing so we are emulating faults in the ``combinational cloud'' before the register. This gives us a reasonable trade-off between complexity and abstraction. In order to perform the injection the netlist of a synthesized circuit has to be modified so that specific nets can be manipulated. We developed a tool that automatically inserts \emph{XOR} gates at every register of a Verilog netlist. Additionally it routes one of the XOR inputs to the toplevel. With this wire a bit can then be simply flipped. The emulation is probability-aware. Instead of injecting single faults into registers and trying to observe their effects on the output, we inject faults at all registers at once based on assigning probabilities. Compared to other implementations our injection is hardware-assisted. To each of these wires, allowing to flip bits, one ``Probabilistic bit-error generator'', is assigned. The purpose of such a generator is to generate a ``1'' based on a assigned probability. Clearly, this hardware assistance results in a large hardware overhead. However it also, allows us to run the emulation at real-time, while maintaining the flexibility to assign one dedicated error-probability to each register in the circuit. Emulation speed is an essential factor, especially as we are injecting faults based on (usually small) probabilities, in order to cover a large state-space in a reasonable amount of time. While the system was initially intended to model radiation based SEUs, we now propose to use it for a variety of approximation methods. Basically all effects can be modeled as an error probability.
\begin{LaTeXdescription}
\item[Voltage Over-scaling] For instance, Voltage-overscaling results in an increased probability of timing violations. The effects as well as the probability of the violation are dependent on the path through the combinational logic. Hence, each combinational logic has to be modeled by a different error probability at the register input.
\item[Circuit Pruning] Circuit pruning, hence, reducing the precision of a circuit by removing parts of it, can also be easily modeled. Assuming a switching activity of $\alpha_{01}=0.5$ we can assign an error probability $p_e=0.5$ to model the pruning of that register. Even if the switching activity does not equal $0.5$, which is usually the case, we can determine the correct $\alpha_{01}$ of that path using simulations and assign the corresponding $p_e=\alpha_{01}$ to it.
\item [Redundancy Omission] And finally if we intend to omit redundancy we can model the effect by assigning the soft-error rate to the register.
\end{LaTeXdescription}
Using the FPGA-based emulation system as a vehicle and the algorithms presented in the following we can determine the maximum error probability of each register, given a required QoS. This information can then be used to evaluate which approximation method can be applied where and to which level.


\subsection{Application-reasoned Approximation}
Clearly it is necessary to reason the required precision of a circuit, i.e. the maximum tolerable error probability of the output pins, from the application that utilizes that particular circuit. We picked out two exemplary applications to clarify this need. Both applications will also be used in Section \ref{sec:experimental_results} to evaluate out methodology. The first application is a generic MIMO wireless communication system. The application exists as a MATLAB model. The model generates random data, transmits it over a noisy communication channel using 8 antennas and a 16 QAM constellation. The signal is received again using 8 antennas, and the receiver tries to recover the original data using a simple ``Zero Forcing Equalizer'' by estimating the communication channel. The equalizer requires to compute the inverse of a $8\times8$ channel matrix. One efficient method to do this involves calculating the ``QR-Decomposition''. In this first example we want to approximate a hardware implementation of QR-Decomposition. The goal is to find a set of operating points for different channels \mbox{$\textrm{SNR [dB]} = \{30; 40; 50\}$} while maintaining a target bit-error rate at the receiver of \mbox{$\textrm{BER}=0.01$}. The second application is a simple ``Sobel Image Filter'', which exists as a regular implementation C. This example differs from the previous one in two major points. First, in this example the input quality is not varying. In this example we want generate approximations for different (in-)accuracies of the filter operation \mbox{$\textrm{PSNR [dB]} = \{30; 40; 50\}$}. And second, the target application is CPU based. I.e. operations would mapped on approximate units in a CPU pipeline. In this application we focus on floating-point operations of a FPU, hence we want to find out what error probability can be allowed at the outputs of a FPU, depending on the operation and the target quality of the sobel filter.

Gathering this information at an algorithmic level unfortunately cannot be automated. Simulation models are required and the required quality of the overall application quality has to be determined. Furthermore for each application, and the type on how the model is implemented, a method to inject faults has to be developed. However, on software-level this is usually a very simple task. Similarly to the algorithm we will introduce in Subsection \ref{subsec:approximation} we propose a two-step approach in order to speed up the simulation. In the first step the integer approximations are determined. Usually this means, that one wants to find out which bits can be ignored. This is a typical fixed-point arithmetic problem, where many methods and tools exist \hl{citation?}. In our two examples, where we only have 2 respectively 3 dimensions this is a simple operation. Starting with the least-significant bit we successively ignored the bits in the model, and observed whether the quality was within the required range. The second step, that we propose is the fine-granular approximation. For the remaining bits, those that cannot be ignored completely, we want to find out what error probability still can be tolerated. In order to do so, we subsequently, register by register, increase the error probability of the output bits. If the quality constraint is satisfied we continue, if not we stop increasing the bit-error rate of that specific output. The result of this approximation at algorithmic level can be seen in Figure \ref{fig:metrics_qr} and \ref{fig:metrics_sobel}. In case of the zero-forcing algorithm, we can see that both for the ``Q'' and the ``R'' part of the decomposition up to 7 bits can be pruned (visible as $p_e=0.5$). This is large number is clearly dependent on the input data and the type of application and might be less for other applications. However, what can also be seen is that there is a dynamics of one bit in both parts, depending on the signal quality. This might seem less in the first place, but on hardware level this multiplies as we will see later. Furthermore, we can see that also for bits that cannot be ignored completely a certain error probability can be tolerated. Even these minor approximations at algorithm level have a large effect on the hardware approximation as we will see later.
\begin{figure}[tb]
  \centering
  \includegraphics[width=.5\textwidth]{figs/metrics_qr}
  \caption{Tolerable imprecision of a QR decomposition, part of a 8x8 MIMO zero-forcing equalizer, for different signal qualities and a target BER=$0.01$}
  \label{fig:metrics_qr}
\end{figure}
For the second application, the sobel filter, we can see similar results. The output bits correspond to a 32-bit IEEE 754 floating-point number. Many bits of the mantissa can be ignored, which is again dependent on that specific application. Exponent bits can never be pruned. In case of \emph{fdiv} and \emph{fmul} the dynamics is very large, which will result in many approximations at hardware level. In case of \emph{fsqrt} no dynamics are possible.
\begin{figure}[tb]
  \centering
  \includegraphics[width=.5\textwidth]{figs/metrics_sobel}
  \caption{Tolerable imprecision of a \emph{sobel} filter for different target qualities (PSNR)}
  \label{fig:metrics_sobel}
\end{figure}


\subsection{Approximation of Sequential Circuits}
\label{subsec:approximation}
With the information generated at the algorithm level, one can now start with the actual hardware approximation. We now know what error probabilities can be tolerated at the circuit output pins for a given required application quality. The next step is to find out which registers in the circuit may have which error probability, while satisfying the output constraint. Hence, we are going one step down in the hierarchy. At first, we thought that the difficulty of performing this step is to find out the one approximation that saves most power from a large set off possible approximations. This is why we started solving this problem using evolutionary algorithms, as it can be formulated as a global optimization problem. However, for us it pointed out not to be the best way. First of all it pointed out to be very unreliable due to the stochastic nature of our problem, as the same set of error applied probabilities leads to different results. Even though they are small, these differences sometimes pushed the algorithms in the wrong direction. And second, the run-time was very long, despite the FPGA-accelerated emulation. The methodology we came up with consists of 4 steps. With each step we are eliminating approximation candidates. This makes the final, fine-grained approximation step manageable concerning its complexity.
\subsubsection{Data-path Separation}
The first step can be seen as a separation of data- and control-path. The idea is to simply exclude all registers from our potential candidates, that have an influence on outputs, where not errors can be tolerated at all. Influencing means that if there is any path from a register to an output, a fault at that register can also be seen at that output. As these outputs are usually control outputs, like address pins, we call this operation data-path separation. This step has to be performed very careful, as every error in the control-path of a circuit, even if it is very unlikely, can have disastrous effects on the functionality of a circuit. The identification of control path-registers is pretty straight-forward. We subsequently inject into each register an error probability of $p_e=0.5$ but only at one at a time, and observe the outputs. If faults could be observed at at least one unwanted output, this register is discarded from the candidates. In order to make this approach prone to false-negative decisions, it ideally has to be performed several times, with a large set of test-vectors.
\subsubsection{High Variance Registers}
The second step pointed out to be necessary in case of the ``QR'' approximation. We observed that the same set of applied error probabilities resulted in a highly varying output error probability. Although the errors could only be observed at data-path outputs, this behavior makes it impossible to reliably approximate a circuit. In order to trust the approximation, the results have to be reproducable, which is why these elements have to be excluded from the list of candidates as well. We are doing this using the same method as for the data-path separation, but this time we are calculating the variance over multiple runs. If the variance is above a certain threshold the register will be discarded.
\subsubsection{Coarse-grained Approximation}
The third step is the first approximation step. In this step we are identifying those registers that can be completely pruned. As a consequence, all elements prior to that, that only influence that one register can be pruned as well. Hence, this is the step that will save most power in the end. As we have excluded all inappropriate elements in the previous steps, this step is straightforward, as shown in Algorithm \ref{alg:coarse_approximation}. The subset of candidates is denoted as the vector $\mathbf{c} = \{c_1,c_2,\ldots,c_{o}\}$.
\begin{algorithm}
  \caption{Coarse Approximation}\label{alg:coarse_approximation}
  \begin{algorithmic}[1]
    \Procedure{CoarseApproximation}{$\mathbf{c},\mathbf{p_{e,max}}$}
    \State $\mathbf{c_{coarse}} = \{\}$
    \For{$i\gets 1,o$}
    \State $\mathbf{p_e(i)} \gets 0.0$
    \EndFor

    \For{$i\gets 1, o$}
    \State $\mathbf{p_e}(i) \gets 0.5$
    \State $\mathbf{p_{e,outputs}}$ = injectFaults($\mathbf{p_e}$)
    \If {$\mathbf{p_{e,outputs}} > \mathbf{p_{e,max}}$}
    \State $\mathbf{p_e}(i) \gets 0.0$
    \Else
    \State appendToVector($\mathbf{c_{coarse}}$,$c_i$)

    \EndIf
    \EndFor

    \EndProcedure
  \end{algorithmic}
\end{algorithm}
We are simply ``switching off'', hence, injecting faults with an error probability of $p_e=0.5$, one register by another and check whether the output error constraint is violated or not. If it is, we switch back on that register, and continue with the next one. Due to the large error probability of the injected faults, this step can usually be performed very fast. Only a few million cycles have to be emulated in order to get a reliable result.
\subsubsection{Fine-grained Approximation}


















\section{Experimental Results}
\label{sec:experimental_results}

\begin{tabular} {| l | l | c | c |}
\hline
Name & Description & Gates & Flip-flops \\
\hline\hline
FIR & 16-tap FIR filter & xx & xx \\
IIR & 8-tap IIR filter & xx &xx \\
DCT4 & 4-input DCT & xx & xx \\
DCT8 & 8-Input DCT & xx & xx \\
fpu100 & 32-bit floating point unit & xx & xx \\
QR & QR decomposition & xx & xx \\
vitdec & Viterbi decoder (131,81) & xx & xx \\
\hline
\end{tabular}
\subsection{Approximation Evaluation}
\hl{power/area table}
\begin{figure}[tb]
  \centering
  \includegraphics[width=.5\textwidth]{figs/optimization_qr}
  \caption{Approximation result for benchmark circuit ``QR'', showing the maximum tolerable error probability at each register}
  \label{fig:optimization_qr}
\end{figure}
\begin{figure}[tb]
  \centering
  \includegraphics[width=.5\textwidth]{figs/optimization_fpu}
  \caption{Approximation result for benchmark circuit ``fpu''}
  \label{fig:optimization_fpu}
\end{figure}
\begin{figure}[tb]
  \centering
  \missingfigure[figwidth=.5\textwidth]{Power viterbi}
  \caption{Possible approximation, in terms of tolerated error proabilities, for benchmark circuit ``viterbi'' for varying signal qualities and a target BER=$0.0$}
  \label{fig:power_viterbi}
\end{figure}


\subsection{Power Evaluation}
\begin{figure}[tb]
  \centering
  \missingfigure[figwidth=.5\textwidth]{Potential Power-savings for benchmar circuit ``QR'' for varying signal qualities}
  \caption{Potential Power-savings for benchmark circuit ``QR'' for varying signal qualities, when peforming 8x8 ZF Equalization}
  \label{fig:power_qr}
\end{figure}
\begin{figure}[tb]
  \centering
  \missingfigure[figwidth=.5\textwidth]{Power FPU}
  \caption{Potential Power-savings for benchmark circuit ``fpu'' for varying target qualities, when performing a sobel filter}
  \label{fig:power_fpu}
\end{figure}
















\section{Conclusion}
\label{sec:conclusion}






\section*{Acknowledgment}





% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section
\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,islped}





% that's all folks
\end{document}
